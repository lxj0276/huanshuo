{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######tensorflow基础\n",
    "#huanshuo0801@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#操作之间的依赖关系\n",
    "a = tf.Variable(1.0,name=\"a\") #定义变量a\n",
    "b = tf.add(a,1,name=\"b\") #定义操作a+1\n",
    "c = tf.add(b,1,name=\"c\") #定义操作b+1\n",
    "d = tf.add(b,10,name=\"d\") #定义操作b+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2\n"
     ]
    }
   ],
   "source": [
    "#会话中执行计算 1+1\n",
    "#构建图，定义两个常量和加法操作\n",
    "v1 = tf.constant(1,name=\"value1\")\n",
    "v2 = tf.constant(1,name=\"value2\")\n",
    "add_op = tf.add(v1,v2,name=\"add_op_name\") #只是定义加法操作，但是并没有执行\n",
    "\n",
    "#在会话中执行操作\n",
    "with tf.Session() as sess:\n",
    "    #执行运算，将Tensorflow中的运算结果赋给python中的变量\n",
    "    result = sess.run(add_op)  #加法结果赋值给result\n",
    "    print(\"1 + 1 = %.0f\" % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of operations : 3\n",
      "operations: \n",
      "name: \"value1\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_INT32\n",
      "      tensor_shape {\n",
      "      }\n",
      "      int_val: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"value2\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_INT32\n",
      "      tensor_shape {\n",
      "      }\n",
      "      int_val: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"add_op_name\"\n",
      "op: \"Add\"\n",
      "input: \"value1\"\n",
      "input: \"value2\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#检查运算\n",
    "tf.reset_default_graph() # 重置图\n",
    "v1 = tf.constant(1,name=\"value1\")\n",
    "v2 = tf.constant(1,name=\"value2\")\n",
    "add_op = tf.add(v1,v2,name=\"add_op_name\") #只是定义加法操作，但是并没有执行\n",
    "\n",
    "graph = tf.get_default_graph() #获取默认图\n",
    "operations = graph.get_operations() #获取图中所有操作\n",
    "#也可以这样写 tf.get_default_graph().get_operations()\n",
    "\n",
    "print(\"number of operations : %d\" % len(operations))\n",
    "print(\"operations: \")\n",
    "for op in operations:\n",
    "    print(op) # 打印操作，可以看到一共有三个操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "#会话中计算的依赖\n",
    "tf.reset_default_graph() # 重置图\n",
    "x = tf.Variable(0.0,name=\"x\") #定义变量x\n",
    "x_plus_1 = tf.assign_add(x, 1, name=\"x_plus\") #定义操作 将x+1后的值赋给x\n",
    "\n",
    "with tf.control_dependencies([x_plus_1]): #依赖x_plus_1的操作\n",
    "    y = tf.identity(x,name='y') #定义操作y 返回一个和x拥有同样的值，相同形状的tensor\n",
    "    \n",
    "init = tf.global_variables_initializer() #初始化所有变量\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(5):\n",
    "        print(y.eval())\n",
    "\n",
    "        \n",
    "#保存图结构，后面可以用tensorboard查看\n",
    "summary_writer = tf.summary.FileWriter('.calc_graph')\n",
    "graph = tf.get_default_graph()\n",
    "summary_writer.add_graph(graph)\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight1 is:\n",
      "0.001\n",
      "Weight2 is:\n",
      "0.002\n"
     ]
    }
   ],
   "source": [
    "#变量的初始化\n",
    "Weight1 = tf.Variable(0.001) #定义变量Weight1\n",
    "Weight2 = tf.Variable(Weight1.initialized_value() * 2) #Weight2的初始值是Weight1的2倍\n",
    "\n",
    "init = tf.global_variables_initializer() #变量必须初始化！\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Weight1 is:\")\n",
    "    print(sess.run(Weight1))\n",
    "    print(\"Weight2 is:\")\n",
    "    print(sess.run(Weight2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v's value is:\n",
      "[1 2 3 4 5 6 7 8 9]\n",
      "reshaped value is:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "#变量的变形\n",
    "v = tf.Variable([1,2,3,4,5,6,7,8,9]) #定一个长度为9的向量\n",
    "\n",
    "reshaped_v = tf.reshape(v,[3,3]) #变成3*3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"v's value is:\")\n",
    "    print(sess.run(v))\n",
    "    print(\"reshaped value is:\")\n",
    "    print(sess.run(reshaped_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(3,)\n",
      "(2,)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "#数据类型和维度\n",
    "#https://www.cnblogs.com/tengge/p/6360946.html\n",
    "value_shape0 = tf.Variable(6666)\n",
    "value_shape1 = tf.Variable([1.1,2.2,3.3])\n",
    "value_shape2 = tf.Variable([1,2],[3,4],[5,6])\n",
    "value_shape3 = tf.Variable([[1],[2],[3]],[[4],[5],[6]]) # 第一维度是2 第二维度是3 第三维度是1\n",
    "\n",
    "print(value_shape0.get_shape())\n",
    "print(value_shape1.get_shape())\n",
    "print(value_shape2.get_shape())\n",
    "print(value_shape3.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 eval loss is 10.528\n",
      "step 10 eval loss is 9.547\n",
      "step 20 eval loss is 9.389\n",
      "step 30 eval loss is 10.254\n",
      "step 40 eval loss is 9.059\n",
      "step 50 eval loss is 8.000\n",
      "step 60 eval loss is 7.318\n",
      "step 70 eval loss is 6.640\n",
      "step 80 eval loss is 6.316\n",
      "step 90 eval loss is 5.268\n",
      "step 100 eval loss is 4.582\n",
      "step 110 eval loss is 4.080\n",
      "step 120 eval loss is 3.630\n",
      "step 130 eval loss is 3.505\n",
      "step 140 eval loss is 2.901\n",
      "step 150 eval loss is 2.201\n",
      "step 160 eval loss is 2.329\n",
      "step 170 eval loss is 2.235\n",
      "step 180 eval loss is 1.798\n",
      "step 190 eval loss is 1.666\n",
      "step 200 eval loss is 1.673\n",
      "step 210 eval loss is 1.919\n",
      "step 220 eval loss is 1.675\n",
      "step 230 eval loss is 1.596\n",
      "step 240 eval loss is 1.357\n",
      "step 250 eval loss is 1.121\n",
      "step 260 eval loss is 1.024\n",
      "step 270 eval loss is 0.865\n",
      "step 280 eval loss is 0.833\n",
      "step 290 eval loss is 0.727\n",
      "step 300 eval loss is 0.709\n",
      "step 310 eval loss is 0.667\n",
      "step 320 eval loss is 0.557\n",
      "step 330 eval loss is 0.533\n",
      "step 340 eval loss is 0.440\n",
      "step 350 eval loss is 0.413\n",
      "step 360 eval loss is 0.450\n",
      "step 370 eval loss is 0.461\n",
      "step 380 eval loss is 0.426\n",
      "step 390 eval loss is 0.442\n",
      "step 400 eval loss is 0.353\n",
      "step 410 eval loss is 0.312\n",
      "step 420 eval loss is 0.316\n",
      "step 430 eval loss is 0.281\n",
      "step 440 eval loss is 0.263\n",
      "step 450 eval loss is 0.225\n",
      "step 460 eval loss is 0.177\n",
      "step 470 eval loss is 0.144\n",
      "step 480 eval loss is 0.149\n",
      "step 490 eval loss is 0.136\n",
      "step 500 eval loss is 0.130\n",
      "step 510 eval loss is 0.112\n",
      "step 520 eval loss is 0.083\n",
      "step 530 eval loss is 0.073\n",
      "step 540 eval loss is 0.066\n",
      "step 550 eval loss is 0.066\n",
      "step 560 eval loss is 0.060\n",
      "step 570 eval loss is 0.075\n",
      "step 580 eval loss is 0.068\n",
      "step 590 eval loss is 0.058\n",
      "step 600 eval loss is 0.049\n",
      "step 610 eval loss is 0.043\n",
      "step 620 eval loss is 0.034\n",
      "step 630 eval loss is 0.029\n",
      "step 640 eval loss is 0.036\n",
      "step 650 eval loss is 0.028\n",
      "step 660 eval loss is 0.024\n",
      "step 670 eval loss is 0.026\n",
      "step 680 eval loss is 0.026\n",
      "step 690 eval loss is 0.023\n",
      "step 700 eval loss is 0.020\n",
      "step 710 eval loss is 0.016\n",
      "step 720 eval loss is 0.014\n",
      "step 730 eval loss is 0.010\n",
      "step 740 eval loss is 0.010\n",
      "step 750 eval loss is 0.010\n",
      "step 760 eval loss is 0.008\n",
      "step 770 eval loss is 0.007\n",
      "step 780 eval loss is 0.005\n",
      "step 790 eval loss is 0.004\n",
      "step 800 eval loss is 0.003\n",
      "step 810 eval loss is 0.004\n",
      "step 820 eval loss is 0.004\n",
      "step 830 eval loss is 0.003\n",
      "step 840 eval loss is 0.003\n",
      "step 850 eval loss is 0.003\n",
      "step 860 eval loss is 0.002\n",
      "step 870 eval loss is 0.002\n",
      "step 880 eval loss is 0.002\n",
      "step 890 eval loss is 0.002\n",
      "step 900 eval loss is 0.002\n",
      "step 910 eval loss is 0.002\n",
      "step 920 eval loss is 0.001\n",
      "step 930 eval loss is 0.001\n",
      "step 940 eval loss is 0.001\n",
      "step 950 eval loss is 0.001\n",
      "step 960 eval loss is 0.000\n",
      "step 970 eval loss is 0.000\n",
      "step 980 eval loss is 0.000\n",
      "step 990 eval loss is 0.001\n"
     ]
    }
   ],
   "source": [
    "############训练Y=weight*X + bias \n",
    "tf.reset_default_graph() # 重置图 一定要加上！\n",
    "def get_data(number):\n",
    "    '''获取训练数据和测试数据'''\n",
    "    list_x = []\n",
    "    list_label = []\n",
    "    for i in range(number):\n",
    "        x = np.random.randn(1) #随机产生x\n",
    "        label = 2 * x + np.random.randn(1) * 0.01 + 10\n",
    "        #将x和label都放入list ，当作样本的特征和label\n",
    "        list_x.append(x) \n",
    "        list_label.append(label)\n",
    "    return list_x,list_label\n",
    "\n",
    "\n",
    "def inference(x):\n",
    "    '''定义网络计算结构 输入x 输出y=weight*x+bias'''\n",
    "    weight = tf.get_variable(\"weight\",[1])\n",
    "    bias = tf.get_variable(\"bias\",[1])\n",
    "    y = x * weight + bias\n",
    "    return y\n",
    "\n",
    "train_x = tf.placeholder(tf.float32)\n",
    "train_label = tf.placeholder(tf.float32)\n",
    "test_x = tf.placeholder(tf.float32)\n",
    "test_label = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"inference\"):\n",
    "    train_y  = inference(train_x)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    test_y = inference(test_x)\n",
    "    \n",
    "'''\n",
    "定义损失函数来计算损失值，这里利用差的平方作为损失函数\n",
    "损失函数是用来计算训练的结果和很是结果的差多少的方法\n",
    "在正常的训练过程中，损失值会越来越少，模型会越来越模拟输入训练数据的分布\n",
    "'''\n",
    "train_loss = tf.square(train_y - train_label)\n",
    "test_loss = tf.square(test_y - test_label)\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(0.002) #采用梯度下降优化函数，根据损失函数的值，确定参数更新方向\n",
    "train_op = opt.minimize(train_loss)\n",
    "\n",
    "init = tf.global_variables_initializer() #参数初始化\n",
    "\n",
    "train_data_x,train_data_label = get_data(1000) #读取训练输出\n",
    "test_data_x,test_data_label = get_data(1)  #读取测试数据\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        #运行一次训练操作\n",
    "        sess.run(train_op,feed_dict={train_x:train_data_x[i],\n",
    "                                     train_label:train_data_label[i]})\n",
    "        if i % 10 == 0:\n",
    "            #每运行10次，测试一下数据\n",
    "            test_loss_value = sess.run(test_loss,\n",
    "                    feed_dict={test_x:test_data_x[0],\n",
    "                               test_label:test_data_label[0]})\n",
    "            print(\"step %d eval loss is %.3f\" % (i,test_loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#研究reuse 作用域\n",
    "with tf.variable_scope(\"root\"):\n",
    "    assert tf.get_variable_scope().reuse == False #可以看出reuse默认是False\n",
    "    with tf.variable_scope(\"foo\"):\n",
    "        assert tf.get_variable_scope().reuse == False # 进入一个子作用域 仍然是False\n",
    "    with tf.variable_scope(\"foo\",reuse = True):\n",
    "        assert tf.get_variable_scope().reuse == True #重新进入一个子作用域 ，强制修改为True\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            assert tf.get_variable_scope().reuse == True  #在内部子作用域，仍是True\n",
    "    assert tf.get_variable_scope().reuse == False  # 跳出子作用域 恢复False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#也可以修改目前变量的作用域为之前已经存在的作用域\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\",[1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\",[1])\n",
    "with tf.variable_scope(foo_scope, reuse=True): \n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    w1 = tf.get_variable(\"w\", [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'foo/v:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'foo/w:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'foo/v:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'foo/w:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(v)\n",
    "print(w)\n",
    "print(v1)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
